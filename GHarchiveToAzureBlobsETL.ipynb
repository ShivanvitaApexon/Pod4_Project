{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ee1c88-f1ce-417e-84d0-35ed5f417929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create execution parameters\n",
    "dbutils.widgets.dropdown(\"monthly\",'false', choices=['true','false'])\n",
    "dbutils.widgets.dropdown(\"daily\",'false', choices=['true','false'])\n",
    "dbutils.widgets.dropdown(\"hourly\",'false', choices=['true','false'])\n",
    "dbutils.widgets.text(\"month\",\"\")\n",
    "dbutils.widgets.text(\"startingDay\",\"\")\n",
    "dbutils.widgets.text(\"endingDay\",\"\")\n",
    "dbutils.widgets.text(\"startingHour\",\"\")\n",
    "dbutils.widgets.text(\"endingHour\",\"\")\n",
    "dbutils.widgets.dropdown(\"eventType\",'push', choices=['push', 'issue', 'all'])\n",
    "dbutils.widgets.text(\"dataContainer\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e7de61-b59e-4bcf-8bb5-4fbb5d8eadb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting azure.storage.blob\n  Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB)\nCollecting typing-extensions>=4.3.0\n  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nCollecting isodate>=0.6.1\n  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.9/site-packages (from azure.storage.blob) (3.4.8)\nCollecting azure-core<2.0.0,>=1.28.0\n  Using cached azure_core-1.28.0-py3-none-any.whl (185 kB)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.28.0->azure.storage.blob) (1.16.0)\nRequirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.28.0->azure.storage.blob) (2.27.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=2.1.4->azure.storage.blob) (1.15.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure.storage.blob) (2.21)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure.storage.blob) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure.storage.blob) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure.storage.blob) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure.storage.blob) (2021.10.8)\nInstalling collected packages: typing-extensions, isodate, azure-core, azure.storage.blob\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-40a979d0-fffe-49eb-a2ba-cfe79ecf801b\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.28.0 azure.storage.blob-12.17.0 isodate-0.6.1 typing-extensions-4.7.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install azure.storage.blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e4c2ce-9bdb-462e-ac00-d92fe11ed0ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from azure.core.pipeline.transport import HttpResponse\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, coalesce, lit, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3e9cf8-a4af-4e50-a152-ede15151be9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL-log-2023-07-14-15-32-43.log\n"
     ]
    }
   ],
   "source": [
    "# Starting logging\n",
    "day_time = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "p_logfile = 'ETL-log-'+day_time+'.log'\n",
    "print(p_logfile)\n",
    "# create logger with 'Custom_log'\n",
    "logger = logging.getLogger('log4j')\n",
    "logger.setLevel(logging.INFO) \n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(p_logfile,mode='a')\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "#setting for ingoring frequest log information\n",
    "#logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "# tell the handler to use this format\n",
    "#fh (file Handler)\n",
    "fh.setFormatter(formatter)\n",
    "#ch (console handler)\n",
    "ch.setFormatter(formatter)\n",
    "#Clearing old frequent log information to ignore that.\n",
    "if (logger.hasHandlers()):\n",
    "     logger.handlers.clear()\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5e4073-81bc-403c-ade0-5f3a7421f1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_data(date, time):\n",
    "\n",
    "    \"\"\"downloads data to {date}-{time}.json.gz file from GHArchive dataset by hitting an API reuqest\n",
    "\n",
    "    Args:\n",
    "        date(str or int) : date of YYYY-MM-DD format.\n",
    "        time(str or int) : hour of the day ranging from 0-23\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    #create filename based on inputs date and time and create url\n",
    "    filename = str(date) +'-'+ str(time)\n",
    "    url = \"https://data.gharchive.org/\"+ filename +'.json.gz'\n",
    "    print(url)\n",
    "    logger.debug('downloading from URL: ' + url)\n",
    "\n",
    "    #send get API request to the url\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check resonse and save gunziped json file in response into a local file on databricks\n",
    "    if response.status_code == 200:\n",
    "\n",
    "       with open(filename +'.json.gz', \"wb\") as file:\n",
    "\n",
    "            file.write(response.content)\n",
    "\n",
    "       print(\"File downloaded successfully.\")\n",
    "       logger.debug('downloaded successfully from URL:' + url)\n",
    "\n",
    "    else:\n",
    "\n",
    "       print(\"Error downloading the file.\")\n",
    "       logger.error('Error downloading from URL:' + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ca8ca7-1b91-455a-b566-f340f4f49190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unzip_jsongz(filename):\n",
    "    \"\"\"Unpacks {filename}.json.gz into {filename}.json\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of gunzip file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    input_file = '/databricks/driver/'+filename+'.json.gz'\n",
    "    output_file = '/databricks/driver/'+filename+'.json'\n",
    "    with gzip.open(input_file, 'rb') as gz_file:\n",
    "        with open(output_file, 'wb') as out_file:\n",
    "            out_file.write(gz_file.read())\n",
    "            logger.debug('Successfully unpacked ' + filename + '.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2466e9f-d631-4373-88d0-0fbc84e58f41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "    \"\"\"Normalizes payload column to the first level \n",
    "\n",
    "    Args:\n",
    "        df (spark dataframe): the  spark df that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe: dataframe with all payload objects converted into json dump strings\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    # creating a list of columns to be selected into the new dataframe\n",
    "    select_exprs = [\n",
    "        col(\"type\"),\n",
    "        col(\"public\"),\n",
    "        col(\"created_at\")\n",
    "    ]\n",
    "    # creating a list of all objects in payload object\n",
    "    payload_columns = [\n",
    "        \"payload.action\",\n",
    "        \"payload.before\",\n",
    "        \"payload.comment\",\n",
    "        \"payload.commits\",\n",
    "        \"payload.description\",\n",
    "        \"payload.distinct_size\",\n",
    "        \"payload.forkee\",\n",
    "        \"payload.head\",\n",
    "        \"payload.issue\",\n",
    "        \"payload.master_branch\",\n",
    "        \"payload.member\",\n",
    "        \"payload.number\",\n",
    "        \"payload.release\",\n",
    "        \"payload.repository_id\",\n",
    "        \"payload.review\",\n",
    "        \"payload.pages\",\n",
    "        \"payload.pull_request\",\n",
    "        \"payload.push_id\",\n",
    "        \"payload.pusher_type\",\n",
    "        \"payload.ref\",\n",
    "        \"payload.size\",\n",
    "        \"payload.ref_type\"\n",
    "    ]\n",
    "    # creating a select expression while casting each of the payload objects into string\n",
    "    for column in payload_columns:\n",
    "        select_expr = col(column).cast(\"string\").alias(column) \n",
    "        #appending select payload columns to the list of columns to be selected       \n",
    "        select_exprs.append(select_expr)\n",
    "    # creating flattened dataframe\n",
    "    flat_df = df.select(select_exprs)\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad4ae5a-95e7-46b7-9a90-296bfbc5991c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def categorize_issue_label(labels,title):\n",
    "    \"\"\" Categorizes issue label into 4 categories: bug, enhancement, request and other, based on the labels and title for the issue event.\n",
    "\n",
    "    Args:\n",
    "        labels (str): label name of the issue event\n",
    "        title (str): title of the issue event\n",
    "    Returns:\n",
    "        target: category of the issue\n",
    "    \"\"\"\n",
    "    #converting labels to lower case\n",
    "    labels = labels.lower()\n",
    "    #matching labels to a catergory based upon its contents\n",
    "    if \"bug\" in labels or \"issue\" in labels:\n",
    "        return \"bug\"\n",
    "    elif \"request\" in labels or \"question\" in labels:\n",
    "        return \"request\"\n",
    "    elif \"enhancement\" in labels or \"feature\" in labels or \"improvement\" in labels:\n",
    "        return \"enhancement\"\n",
    "    \n",
    "    #converting title to lower case\n",
    "    title = title.lower()\n",
    "    #matching title to a catergory based upon its contents\n",
    "    if \"bug\" in title or \"issue\" in title:\n",
    "        return \"bug\"\n",
    "    elif \"request\" in title or \"question\" in title:\n",
    "        return \"request\"\n",
    "    elif \"enhancement\" in title or \"feature\" in title or \"improvement\" in title:\n",
    "        return \"enhancement\"\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8daa2d8-2d59-4b77-a9ba-15cd611ba098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_issues_df(issues_df):\n",
    "    \"\"\"Normalizes payload column by selecting necessary columns for issuesEvent\n",
    "\n",
    "    Args:\n",
    "        df (spark dataframe): the spark df that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe: dataframe with all issues relevant payload objects placed in seperate columns\n",
    "    \"\"\"\n",
    "    # creating a dataframe out of all necessary objects in payload object related to issues event\n",
    "    issues_flat_df = issues_df.select(\n",
    "    col(\"repo.name\").alias(\"repo_name\"),\n",
    "    col(\"payload.issue.html_url\").alias(\"html_url\"),\n",
    "    col(\"payload.issue.title\").alias(\"title\"),\n",
    "    col(\"payload.issue.body\").alias(\"body\"),\n",
    "    explode(col(\"payload.issue.labels\")).alias(\"labels\"))\n",
    "    #extracting label names into target column\n",
    "    issues_flat_df = issues_flat_df.withColumn(\"target\", col(\"labels.name\"))\n",
    "    #categorizing target column\n",
    "    categorize_udf = udf(categorize_issue_label, StringType())\n",
    "    issues_flat_df = issues_flat_df.withColumn(\"target\", categorize_udf(issues_flat_df[\"target\"], issues_flat_df[\"title\"]))\n",
    "\n",
    "    return issues_flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d352ae1c-8315-4664-96f5-bb2fd6702e39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_push_df(push_df):\n",
    "    \"\"\"Normalizes payload column by selecting necessary columns for pushEvent\n",
    "\n",
    "    Args:\n",
    "        df (spark dataframe): the spark df that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe: dataframe with all push event relevant payload objects placed in seperate columns\n",
    "    \"\"\"\n",
    "    columns = push_df.columns\n",
    "    # creating a list of columns to be selected into the new dataframe\n",
    "    select_exprs = [\n",
    "        col(\"type\"),\n",
    "        col(\"public\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"org\"),\n",
    "        col(\"actor\"),\n",
    "        col(\"repo\")\n",
    "    ]\n",
    "    # creating a list of all objects in payload object relevant to push event\n",
    "    payload_columns = [\n",
    "        \"payload.action\",\n",
    "        \"payload.before\",\n",
    "        \"payload.commits\",\n",
    "        \"payload.repository_id\",\n",
    "        \"payload.review\",\n",
    "        \"payload.pages\",\n",
    "        \"payload.push_id\",\n",
    "        \"payload.pusher_type\",\n",
    "        \"payload.ref\",\n",
    "        \"payload.size\",\n",
    "        \"payload.ref_type\"\n",
    "    ]\n",
    "    \n",
    "    for column in payload_columns:\n",
    "        #creating a select expression for each of the payload objects\n",
    "        select_expr = col(column).alias(column) \n",
    "        #appending select payload columns to the list of columns to be selected       \n",
    "        select_exprs.append(select_expr)\n",
    "    # creating flattened dataframe\n",
    "    flat_push_df = push_df.select(select_exprs)\n",
    "    return flat_push_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377662b3-ef3e-465b-9c5b-9c0b1ce2d52c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_to_blob_storage( connection_string, container_name, file_path, blob_name):\n",
    "    \"\"\"upload file to blob storage\n",
    "\n",
    "    Args:\n",
    "        connection_string (str): Azure blob connection string\n",
    "        container_name (str): Azure blob container name\n",
    "        file_path (str): The file location of file to upload to blob storage\n",
    "        blob_name (str): The location on blob storage to upload the file to\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #fetching blob_service_client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    #fetching blob_service_client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    #checking if storage container exists\n",
    "    if not container_client.exists():\n",
    "        #creating a storage container if it doesn't exist\n",
    "        container_client.create_container()\n",
    "    #fetching blob_client\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    #uploading to blob storage\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f39ad8-0cbd-4d23-8bd7-72a8afa010fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clear_container_data(connection_string, container_name):\n",
    "    \"\"\"Clear all data in an Azure Blob Storage container.\n",
    "\n",
    "    Args:\n",
    "        connection_string (str): Azure Blob Storage connection string.\n",
    "        container_name (str): Azure Blob Storage container name.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #fetching blob_service_client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    #fetching container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    #fetching files with in the storage container\n",
    "    blobs = container_client.list_blobs()\n",
    "    #deleting the fetched files in the storage container\n",
    "    for blob in blobs:\n",
    "        container_client.delete_blob(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272b9fb9-d212-4e05-92ce-c8ab57c44637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def etl(monthly,hourly,daily,month,startingDay,endingDay,startingHour,endingHour,eventType = 'push', container_name = 'data'):\n",
    "    \"\"\"Downloads, transforms and uploads data in parquet format to blob storage\n",
    "\n",
    "    Args:\n",
    "        monthly (str): string 'fale' or 'true'. fetch a month's data.\n",
    "        hourly (str): string 'fale' or 'true'. fetch data in an hour range of a day.\n",
    "        daily (str): string 'fale' or 'true'. fetch data in a day range of a month.\n",
    "        month (str): month to fetch the data from. YYYY-MM format.\n",
    "        startingDay (str): starting day to fecth data from. 1-30 range.\n",
    "        endingDay (str): ending day to fecth data from. 1-30 range.\n",
    "        startingHour (str): starting hour to fecth data from. 0-23 range.\n",
    "        endingHour (str): ending hour to fecth data from. 0-23 range.\n",
    "        eventType (str, Optional): Type of events to be fetched.\n",
    "        container_name (str, Optional): Azure storage container name to store the data in.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #initializes spark context\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    #initializing constants\n",
    "    cache_folder_path_spark = \"file:///databricks/driver/cache\"\n",
    "    cache_folder_path = \"/databricks/driver/cache\"\n",
    "    folder_path = \"/databricks/driver/\"\n",
    "    connection_string = \"DefaultEndpointsProtocol=https;AccountName=pod4projectstorage;AccountKey=2hClDrVPLGX4QBDBk8OylAkHqczIQfDja66Yl488rmj/0+vb+CAzOxL5qMe5XyM9ZupgwveVRm3N+AStriO5vg==;EndpointSuffix=core.windows.net\"\n",
    "    #clearing container data\n",
    "    clear_container_data(connection_string,container_name)\n",
    "    logger.debug('Cleared container data')\n",
    "\n",
    "    dayRange = None\n",
    "    hourRange = None\n",
    "    event_type = None\n",
    "    flatten = None\n",
    "    total_download_size = 0\n",
    "\n",
    "    #setting day and hour ranges to fecth the data from\n",
    "    if monthly:\n",
    "        dayRange = range(1,31)\n",
    "        hourRange = range(0,24)\n",
    "    elif daily:\n",
    "        dayRange = range(int(startingDay), int(endingDay)+1)\n",
    "        hourRange = range(0,24)\n",
    "    elif hourly:\n",
    "        dayRange = range(int(startingDay), int(startingDay)+1)\n",
    "        hourRange = range(int(startingHour),int(endingHour)+1)\n",
    "\n",
    "    # Setting the type of events to filter and respective flatten method\n",
    "    if eventType == 'push':\n",
    "        event_type = 'PushEvent'\n",
    "        flatten = flatten_push_df\n",
    "    elif eventType == 'issue':\n",
    "        event_type = 'IssuesEvent'\n",
    "        flatten = flatten_issues_df\n",
    "    elif eventType == 'all':\n",
    "        event_type = 'all'\n",
    "        flatten = flatten_df\n",
    "    #looping over day range\n",
    "    for day in dayRange:\n",
    "        #formatting day to a DD format as required by GHArchives\n",
    "        day_str = \"{:02}\".format(day)\n",
    "        #initializing an empty df to store at most a days data\n",
    "        main_df = None\n",
    "        parquet_filename = None\n",
    "        #looping over hour range\n",
    "        for hour in hourRange:\n",
    "            filename = month +\"-\" + day_str +\"-\" + str(hour)\n",
    "            #fetching an hours data\n",
    "            get_data(month +\"-\"+day_str, hour)\n",
    "            #unpacking the hourly data\n",
    "            unzip_jsongz(filename)\n",
    "            #reading json data into a spark df\n",
    "            df = spark.read.json(\"file:///databricks/driver/\"+filename+\".json\")\n",
    "            #filtering required events\n",
    "            if event_type != 'all':\n",
    "                df = df.filter(df.type == event_type)\n",
    "            #flattening df\n",
    "            flat_df = flatten(df)\n",
    "            #combining hourly data into main df's daily data\n",
    "            if main_df is not None:\n",
    "                main_df = main_df.unionAll(flat_df)\n",
    "            else:\n",
    "                main_df = flat_df \n",
    "        #Clearing cache folder containing parquet file from previous loop in the day range\n",
    "        if os.path.exists(cache_folder_path):\n",
    "            os.system(\"rm -rf {}\".format(cache_folder_path))\n",
    "            print(\"cache cleared\")\n",
    "            logger.debug('databricks cache cleared')\n",
    "        #creating a single parquet file in the cache folder\n",
    "        main_df.coalesce(1).write.parquet(cache_folder_path_spark)\n",
    "        #fecthing parquet file created by spark in cache folder\n",
    "        parquet_files = Path(cache_folder_path).glob(\"*.parquet\")\n",
    "        #fetching name of the parquet file\n",
    "        for file in parquet_files:\n",
    "            parquet_filename = file\n",
    "            print(parquet_filename)\n",
    "        #creating blob name\n",
    "        blob_name = month +\"-\"+day_str+\".snappy.parquet\"\n",
    "        #uploading to blob storage\n",
    "        upload_to_blob_storage( connection_string, container_name, parquet_filename, blob_name)\n",
    "        print(\"uploaded {} to blob storage\".format(blob_name))\n",
    "        #fetching parquet file size\n",
    "        file_size = os.path.getsize(parquet_filename)\n",
    "        print(\"File size: \", int(file_size), \" bytes\")\n",
    "        #logging file size of the blob\n",
    "        logger.info(blob_name+' of size '+str(file_size)+' successfully uploaded to blob storage')\n",
    "        #adding current file size to the total download size\n",
    "        total_download_size += int(file_size)\n",
    "        logger.info('Current total download size: ' + str(total_download_size))\n",
    "        #removing all local json files to clear memory space\n",
    "        json_files = glob.glob(os.path.join(folder_path, \"*.json*\"))\n",
    "        for file_path in json_files:\n",
    "            os.remove(file_path)\n",
    "            print(\"File '{}' removed.\".format(file_path))\n",
    "            logger.debug(\"File '{}' removed.\".format(file_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bebc97f-057a-43a7-ba15-d6e0849876c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fetch execution parameters\n",
    "monthly = dbutils.widgets.get(\"monthly\")\n",
    "monthly = False if monthly == 'false' else bool(monthly)\n",
    "daily = dbutils.widgets.get(\"daily\")\n",
    "daily = False if daily == 'false' else bool(daily)\n",
    "hourly = dbutils.widgets.get(\"hourly\")\n",
    "hourly = False if hourly == 'false' else bool(hourly)\n",
    "month = dbutils.widgets.get(\"month\")\n",
    "startingDay = dbutils.widgets.get(\"startingDay\")\n",
    "endingDay = dbutils.widgets.get(\"endingDay\")\n",
    "startingHour = dbutils.widgets.get(\"startingHour\")\n",
    "endingHour = dbutils.widgets.get(\"endingHour\")\n",
    "eventType = dbutils.widgets.get(\"eventType\")\n",
    "container_name = dbutils.widgets.get(\"dataContainer\")\n",
    "#setting container name as data by default if it is null or empty\n",
    "if container_name is None or container_name.strip() == \"\":\n",
    "    container_name = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1104d0-5569-4ae4-bf0d-fbe3477da779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.gharchive.org/2023-06-06-4.json.gz\nFile downloaded successfully.\nhttps://data.gharchive.org/2023-06-06-5.json.gz\nFile downloaded successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 15:34:21,024 - log4j - INFO - 2023-06-06.snappy.parquet of size 1541085 successfully uploaded to blob storage\n2023-07-14 15:34:21,025 - log4j - INFO - Current total download size: 1541085\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/databricks/driver/cache/part-00000-tid-8000346145254716032-e721d24f-dd76-447e-b49d-991c967ad790-25-1-c000.snappy.parquet\nuploaded 2023-06-06.snappy.parquet to blob storage\nFile size:  1541085  bytes\nFile '/databricks/driver/2023-06-06-4.json' removed.\nFile '/databricks/driver/2023-06-06-5.json' removed.\nFile '/databricks/driver/2023-06-06-5.json.gz' removed.\nFile '/databricks/driver/2023-06-06-4.json.gz' removed.\n"
     ]
    }
   ],
   "source": [
    "#Executing ETL\n",
    "etl(bool(monthly),bool(hourly),bool(daily),month,startingDay,endingDay,startingHour,endingHour,eventType,container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c667f24-03e8-4f8e-ad50-0d525ddcc1ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating log file\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ae8ed6-b369-4468-a8d9-d5b4eb1d6463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Storing logs in blob storage\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=pod4projectstorage;AccountKey=2hClDrVPLGX4QBDBk8OylAkHqczIQfDja66Yl488rmj/0+vb+CAzOxL5qMe5XyM9ZupgwveVRm3N+AStriO5vg==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"logs\"\n",
    "file_path = p_logfile\n",
    "blob_name = p_logfile\n",
    "upload_to_blob_storage( connection_string, container_name, file_path, blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bef5cf-5278-4e2a-b93e-1895cebf2d25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#close all widgets\n",
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbf5ea6f-95c8-41c9-bdd3-699032b30992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GHarchiveToAzureBlobsETL",
   "widgets": {
    "daily": {
     "currentValue": "false",
     "nuid": "17ee4da9-083d-4711-abb3-63b3ffeaf02f",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "daily",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "dataContainer": {
     "currentValue": "issues",
     "nuid": "65e62beb-55c9-4978-b283-1d2f48a59a8c",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataContainer",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "endingDay": {
     "currentValue": "6",
     "nuid": "842fc6e4-622a-498a-af4a-d7a4077dec99",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "endingDay",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "endingHour": {
     "currentValue": "5",
     "nuid": "c8577dcb-80a1-4faa-9c64-19a721ce754e",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "endingHour",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "eventType": {
     "currentValue": "issue",
     "nuid": "9f2917a3-826c-41cd-afc2-9b8dbe797462",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "push",
      "label": null,
      "name": "eventType",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "push",
        "issue",
        "all"
       ]
      }
     }
    },
    "hourly": {
     "currentValue": "true",
     "nuid": "8b523aa2-5343-4336-885e-db950ed8d4b8",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "hourly",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "month": {
     "currentValue": "2023-06",
     "nuid": "11687fe5-c880-4a93-916e-79acedb400f3",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "month",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "monthly": {
     "currentValue": "false",
     "nuid": "908c1881-2766-45cc-8b67-b6696c9918a7",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "monthly",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "startingDay": {
     "currentValue": "6",
     "nuid": "80b99401-4292-4263-9ecc-79b5b91cf82d",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "startingDay",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "startingHour": {
     "currentValue": "4",
     "nuid": "2f91ebba-213c-4626-a530-853d81511d2d",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "startingHour",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
