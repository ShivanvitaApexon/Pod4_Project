{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ee1c88-f1ce-417e-84d0-35ed5f417929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create execution parameters\n",
    "dbutils.widgets.dropdown(\"monthly\",'false', choices=['true','false'])\n",
    "dbutils.widgets.dropdown(\"daily\",'false', choices=['true','false'])\n",
    "dbutils.widgets.dropdown(\"hourly\",'false', choices=['true','false'])\n",
    "dbutils.widgets.text(\"month\",\"\")\n",
    "dbutils.widgets.text(\"startingDay\",\"\")\n",
    "dbutils.widgets.text(\"endingDay\",\"\")\n",
    "dbutils.widgets.text(\"startingHour\",\"\")\n",
    "dbutils.widgets.text(\"endingHour\",\"\")\n",
    "dbutils.widgets.dropdown(\"eventType\",'push', choices=['push', 'issue', 'all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e7de61-b59e-4bcf-8bb5-4fbb5d8eadb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install azure.storage.blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e4c2ce-9bdb-462e-ac00-d92fe11ed0ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from azure.core.pipeline.transport import HttpResponse\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, coalesce, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3e9cf8-a4af-4e50-a152-ede15151be9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Starting logging\n",
    "day_time = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "p_logfile = 'ETL-log-'+day_time+'.log'\n",
    "print(p_logfile)\n",
    "# create logger with 'Custom_log'\n",
    "logger = logging.getLogger('log4j')\n",
    "logger.setLevel(logging.INFO) \n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(p_logfile,mode='a')\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "#setting for ingoring frequest log information\n",
    "#logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "# tell the handler to use this format\n",
    "#fh (file Handler)\n",
    "fh.setFormatter(formatter)\n",
    "#ch (console handler)\n",
    "ch.setFormatter(formatter)\n",
    "#Clearing old frequent log information to ignore that.\n",
    "if (logger.hasHandlers()):\n",
    "     logger.handlers.clear()\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5e4073-81bc-403c-ade0-5f3a7421f1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data(date, time):\n",
    "\n",
    "    \"\"\"downloads data to {date}-{time}.json.gz file from GHArchive dataset by hitting an API reuqest\n",
    "\n",
    "    Args:\n",
    "        date(str or int) : date of YYYY-MM-DD format.\n",
    "        time(str or int) : hour of the day ranging from 0-23\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    #create filename based on inputs date and time and create url\n",
    "    filename = str(date) +'-'+ str(time)\n",
    "    url = \"https://data.gharchive.org/\"+ filename +'.json.gz'\n",
    "    print(url)\n",
    "    logger.debug('downloading from URL: ' + url)\n",
    "\n",
    "    #send get API request to the url\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check resonse and save gunziped json file in response into a local file on databricks\n",
    "    if response.status_code == 200:\n",
    "\n",
    "       with open(filename +'.json.gz', \"wb\") as file:\n",
    "\n",
    "            file.write(response.content)\n",
    "\n",
    "       print(\"File downloaded successfully.\")\n",
    "       logger.debug('downloaded successfully from URL:' + url)\n",
    "\n",
    "    else:\n",
    "\n",
    "       print(\"Error downloading the file.\")\n",
    "       logger.error('Error downloading from URL:' + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ca8ca7-1b91-455a-b566-f340f4f49190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unzip_jsongz(filename):\n",
    "    \"\"\"Unpacks {filename}.json.gz into {filename}.json\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of gunzip file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    input_file = '/databricks/driver/'+filename+'.json.gz'\n",
    "    output_file = '/databricks/driver/'+filename+'.json'\n",
    "    with gzip.open(input_file, 'rb') as gz_file:\n",
    "        with open(output_file, 'wb') as out_file:\n",
    "            out_file.write(gz_file.read())\n",
    "            logger.debug('Successfully unpacked ' + filename + '.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2466e9f-d631-4373-88d0-0fbc84e58f41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "    \"\"\"Normalizes payload column to the first level \n",
    "\n",
    "    Args:\n",
    "        df (spark dataframe): the  spark df that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe: dataframe with all payload objects converted into json dump strings\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    # creating a list of columns to be selected into the new dataframe\n",
    "    select_exprs = [\n",
    "        col(\"type\"),\n",
    "        col(\"public\"),\n",
    "        col(\"created_at\")\n",
    "    ]\n",
    "    # creating a list of all objects in payload object\n",
    "    payload_columns = [\n",
    "        \"payload.action\",\n",
    "        \"payload.before\",\n",
    "        \"payload.comment\",\n",
    "        \"payload.commits\",\n",
    "        \"payload.description\",\n",
    "        \"payload.distinct_size\",\n",
    "        \"payload.forkee\",\n",
    "        \"payload.head\",\n",
    "        \"payload.issue\",\n",
    "        \"payload.master_branch\",\n",
    "        \"payload.member\",\n",
    "        \"payload.number\",\n",
    "        \"payload.release\",\n",
    "        \"payload.repository_id\",\n",
    "        \"payload.review\",\n",
    "        \"payload.pages\",\n",
    "        \"payload.pull_request\",\n",
    "        \"payload.push_id\",\n",
    "        \"payload.pusher_type\",\n",
    "        \"payload.ref\",\n",
    "        \"payload.size\",\n",
    "        \"payload.ref_type\"\n",
    "    ]\n",
    "    # creating a select expression while casting each of the payload objects into string\n",
    "    for column in payload_columns:\n",
    "        select_expr = col(column).cast(\"string\").alias(column) \n",
    "        #appending select payload columns to the list of columns to be selected       \n",
    "        select_exprs.append(select_expr)\n",
    "    # creating flattened dataframe\n",
    "    flat_df = df.select(select_exprs)\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8daa2d8-2d59-4b77-a9ba-15cd611ba098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_issues_df(issues_df):\n",
    "    \"\"\"Normalizes payload column by selecting necessary columns for issuesEvent\n",
    "\n",
    "    Args:\n",
    "        df (spark dataframe): the spark df that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe: dataframe with all issues relevant payload objects placed in seperate columns\n",
    "    \"\"\"\n",
    "    # creating a dataframe out of all necessary objects in payload object related to issues event\n",
    "    issues_flat_df = issues_df.select(\n",
    "    col(\"type\"),\n",
    "    col(\"public\"),\n",
    "    col(\"created_at\"),\n",
    "    col(\"payload.action\").alias(\"payload.action\"),\n",
    "    col(\"payload.issue.url\").alias(\"payload.issue.url\"),\n",
    "    col(\"payload.issue.number\").alias(\"payload.issue.number\"),\n",
    "    col(\"payload.issue.title\").alias(\"payload.issue.title\"),\n",
    "    col(\"payload.issue.user.login\").alias(\"payload.issue.user.login\"),\n",
    "    col(\"payload.issue.user.type\").alias(\"payload.issue.user.type\"),\n",
    "    col(\"payload.issue.user.site_admin\").alias(\"payload.issue.user.site_admin\"),\n",
    "    col(\"payload.issue.labels\").alias(\"payload.issue.labels\"),\n",
    "    col(\"payload.issue.state\").alias(\"payload.issue.state\"),\n",
    "    col(\"payload.issue.locked\").alias(\"payload.issue.locked\"),\n",
    "    col(\"payload.issue.comments\").alias(\"payload.issue.comments\"),\n",
    "    col(\"payload.issue.created_at\").alias(\"payload.issue.created_at\"),\n",
    "    col(\"payload.issue.updated_at\").alias(\"payload.issue.updated_at\"),\n",
    "    col(\"payload.issue.closed_at\").alias(\"payload.issue.closed_at\"),\n",
    "    col(\"payload.issue.author_association\").alias(\"payload.issue.author_association\"),\n",
    "    col(\"payload.issue.active_lock_reason\").alias(\"payload.issue.active_lock_reason\"),\n",
    "    col(\"payload.issue.body\").alias(\"payload.issue.body\"),\n",
    "    col(\"payload.issue.state_reason\").alias(\"payload.issue.state_reason\"))\n",
    "    return issues_flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d352ae1c-8315-4664-96f5-bb2fd6702e39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_push_df(push_df):\n",
    "    \"\"\"Normalizes payload column by selecting necessary columns for pushEvent\n",
    "\n",
    "    Args:\n",
    "        df (spark dataframe): the spark df that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe: dataframe with all push event relevant payload objects placed in seperate columns\n",
    "    \"\"\"\n",
    "    columns = push_df.columns\n",
    "    # creating a list of columns to be selected into the new dataframe\n",
    "    select_exprs = [\n",
    "        col(\"type\"),\n",
    "        col(\"public\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"org\"),\n",
    "        col(\"actor\"),\n",
    "        col(\"repo\")\n",
    "    ]\n",
    "    # creating a list of all objects in payload object relevant to push event\n",
    "    payload_columns = [\n",
    "        \"payload.action\",\n",
    "        \"payload.before\",\n",
    "        \"payload.commits\",\n",
    "        \"payload.repository_id\",\n",
    "        \"payload.review\",\n",
    "        \"payload.pages\",\n",
    "        \"payload.push_id\",\n",
    "        \"payload.pusher_type\",\n",
    "        \"payload.ref\",\n",
    "        \"payload.size\",\n",
    "        \"payload.ref_type\"\n",
    "    ]\n",
    "    \n",
    "    for column in payload_columns:\n",
    "        #creating a select expression for each of the payload objects\n",
    "        select_expr = col(column).alias(column) \n",
    "        #appending select payload columns to the list of columns to be selected       \n",
    "        select_exprs.append(select_expr)\n",
    "    # creating flattened dataframe\n",
    "    flat_push_df = push_df.select(select_exprs)\n",
    "    return flat_push_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377662b3-ef3e-465b-9c5b-9c0b1ce2d52c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_to_blob_storage( connection_string, container_name, file_path, blob_name):\n",
    "    \"\"\"upload file to blob storage\n",
    "\n",
    "    Args:\n",
    "        connection_string (str): Azure blob connection string\n",
    "        container_name (str): Azure blob container name\n",
    "        file_path (str): The file location of file to upload to blob storage\n",
    "        blob_name (str): The location on blob storage to upload the file to\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #fetching blob_service_client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    #fetching blob_service_client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    #checking if storage container exists\n",
    "    if not container_client.exists():\n",
    "        #creating a storage container if it doesn't exist\n",
    "        container_client.create_container()\n",
    "    #fetching blob_client\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    #uploading to blob storage\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f39ad8-0cbd-4d23-8bd7-72a8afa010fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clear_container_data(connection_string, container_name):\n",
    "    \"\"\"Clear all data in an Azure Blob Storage container.\n",
    "\n",
    "    Args:\n",
    "        connection_string (str): Azure Blob Storage connection string.\n",
    "        container_name (str): Azure Blob Storage container name.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #fetching blob_service_client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    #fetching container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    #fetching files with in the storage container\n",
    "    blobs = container_client.list_blobs()\n",
    "    #deleting the fetched files in the storage container\n",
    "    for blob in blobs:\n",
    "        container_client.delete_blob(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272b9fb9-d212-4e05-92ce-c8ab57c44637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def etl(monthly,hourly,daily,month,startingDay,endingDay,startingHour,endingHour,eventType = 'push'):\n",
    "    \"\"\"Downloads, transforms and uploads data in parquet format to blob storage\n",
    "\n",
    "    Args:\n",
    "        monthly (str): string 'fale' or 'true'. fetch a month's data.\n",
    "        hourly (str): string 'fale' or 'true'. fetch data in an hour range of a day.\n",
    "        daily (str): string 'fale' or 'true'. fetch data in a day range of a month.\n",
    "        month (str): month to fetch the data from. YYYY-MM format.\n",
    "        startingDay (str): starting day to fecth data from. 1-30 range.\n",
    "        endingDay (str): ending day to fecth data from. 1-30 range.\n",
    "        startingHour (str): starting hour to fecth data from. 0-23 range.\n",
    "        endingHour (str): ending hour to fecth data from. 0-23 range.\n",
    "        eventType (str, Optional): Type of events to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #initializes spark context\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    #initializing constants\n",
    "    cache_folder_path_spark = \"file:///databricks/driver/cache\"\n",
    "    cache_folder_path = \"/databricks/driver/cache\"\n",
    "    folder_path = \"/databricks/driver/\"\n",
    "    connection_string = \"DefaultEndpointsProtocol=https;AccountName=pod4projectstorage;AccountKey=2hClDrVPLGX4QBDBk8OylAkHqczIQfDja66Yl488rmj/0+vb+CAzOxL5qMe5XyM9ZupgwveVRm3N+AStriO5vg==;EndpointSuffix=core.windows.net\"\n",
    "    container_name = \"data\"\n",
    "    #clearing container data\n",
    "    clear_container_data(connection_string,container_name)\n",
    "    logger.debug('Cleared container data')\n",
    "\n",
    "    dayRange = None\n",
    "    hourRange = None\n",
    "    event_type = None\n",
    "    flatten = None\n",
    "    total_download_size = 0\n",
    "\n",
    "    #setting day and hour ranges to fecth the data from\n",
    "    if monthly:\n",
    "        dayRange = range(1,31)\n",
    "        hourRange = range(0,24)\n",
    "    elif daily:\n",
    "        dayRange = range(int(startingDay), int(endingDay)+1)\n",
    "        hourRange = range(0,24)\n",
    "    elif hourly:\n",
    "        dayRange = range(int(startingDay), int(startingDay)+1)\n",
    "        hourRange = range(int(startingHour),int(endingHour)+1)\n",
    "\n",
    "    # Setting the type of events to filter and respective flatten method\n",
    "    if eventType == 'push':\n",
    "        event_type = 'PushEvent'\n",
    "        flatten = flatten_push_df\n",
    "    elif eventType == 'issue':\n",
    "        event_type = 'IssuesEvent'\n",
    "        flatten = flatten_issues_df\n",
    "    elif eventType == 'all':\n",
    "        event_type = 'all'\n",
    "        flatten = flatten_df\n",
    "    #looping over day range\n",
    "    for day in dayRange:\n",
    "        #formatting day to a DD format as required by GHArchives\n",
    "        day_str = \"{:02}\".format(day)\n",
    "        #initializing an empty df to store at most a days data\n",
    "        main_df = None\n",
    "        parquet_filename = None\n",
    "        #looping over hour range\n",
    "        for hour in hourRange:\n",
    "            filename = month +\"-\" + day_str +\"-\" + str(hour)\n",
    "            #fetching an hours data\n",
    "            get_data(month +\"-\"+day_str, hour)\n",
    "            #unpacking the hourly data\n",
    "            unzip_jsongz(filename)\n",
    "            #reading json data into a spark df\n",
    "            df = spark.read.json(\"file:///databricks/driver/\"+filename+\".json\")\n",
    "            #filtering required events\n",
    "            if event_type != 'all':\n",
    "                df = df.filter(df.type == event_type)\n",
    "            #flattening df\n",
    "            flat_df = flatten(df)\n",
    "            #combining hourly data into main df's daily data\n",
    "            if main_df is not None:\n",
    "                main_df = main_df.unionAll(flat_df)\n",
    "            else:\n",
    "                main_df = flat_df \n",
    "        #Clearing cache folder containing parquet file from previous loop in the day range\n",
    "        if os.path.exists(cache_folder_path):\n",
    "            os.system(\"rm -rf {}\".format(cache_folder_path))\n",
    "            print(\"cache cleared\")\n",
    "            logger.debug('databricks cache cleared')\n",
    "        #creating a single parquet file in the cache folder\n",
    "        main_df.coalesce(1).write.parquet(cache_folder_path_spark)\n",
    "        #fecthing parquet file created by spark in cache folder\n",
    "        parquet_files = Path(cache_folder_path).glob(\"*.parquet\")\n",
    "        #fetching name of the parquet file\n",
    "        for file in parquet_files:\n",
    "            parquet_filename = file\n",
    "            print(parquet_filename)\n",
    "        #creating blob name\n",
    "        blob_name = month +\"-\"+day_str+\".snappy.parquet\"\n",
    "        #uploading to blob storage\n",
    "        upload_to_blob_storage( connection_string, container_name, parquet_filename, blob_name)\n",
    "        print(\"uploaded {} to blob storage\".format(blob_name))\n",
    "        #fetching parquet file size\n",
    "        file_size = os.path.getsize(parquet_filename)\n",
    "        print(\"File size: \", int(file_size), \" bytes\")\n",
    "        #logging file size of the blob\n",
    "        logger.info(blob_name+' of size '+str(file_size)+' successfully uploaded to blob storage')\n",
    "        #adding current file size to the total download size\n",
    "        total_download_size += int(file_size)\n",
    "        logger.info('Current total download size: ' + str(total_download_size))\n",
    "        #removing all local json files to clear memory space\n",
    "        json_files = glob.glob(os.path.join(folder_path, \"*.json*\"))\n",
    "        for file_path in json_files:\n",
    "            os.remove(file_path)\n",
    "            print(\"File '{}' removed.\".format(file_path))\n",
    "            logger.debug(\"File '{}' removed.\".format(file_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bebc97f-057a-43a7-ba15-d6e0849876c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fetch execution parameters\n",
    "monthly = dbutils.widgets.get(\"monthly\")\n",
    "monthly = False if monthly == 'false' else bool(monthly)\n",
    "daily = dbutils.widgets.get(\"daily\")\n",
    "daily = False if daily == 'false' else bool(daily)\n",
    "hourly = dbutils.widgets.get(\"hourly\")\n",
    "hourly = False if hourly == 'false' else bool(hourly)\n",
    "month = dbutils.widgets.get(\"month\")\n",
    "startingDay = dbutils.widgets.get(\"startingDay\")\n",
    "endingDay = dbutils.widgets.get(\"endingDay\")\n",
    "startingHour = dbutils.widgets.get(\"startingHour\")\n",
    "endingHour = dbutils.widgets.get(\"endingHour\")\n",
    "eventType = dbutils.widgets.get(\"eventType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1104d0-5569-4ae4-bf0d-fbe3477da779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Executing ETL\n",
    "etl(bool(monthly),bool(hourly),bool(daily),month,startingDay,endingDay,startingHour,endingHour,eventType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c667f24-03e8-4f8e-ad50-0d525ddcc1ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating log file\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ae8ed6-b369-4468-a8d9-d5b4eb1d6463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Storing logs in blob storage\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=pod4projectstorage;AccountKey=2hClDrVPLGX4QBDBk8OylAkHqczIQfDja66Yl488rmj/0+vb+CAzOxL5qMe5XyM9ZupgwveVRm3N+AStriO5vg==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"logs\"\n",
    "file_path = p_logfile\n",
    "blob_name = p_logfile\n",
    "upload_to_blob_storage( connection_string, container_name, file_path, blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bef5cf-5278-4e2a-b93e-1895cebf2d25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#close all widgets\n",
    "dbutils.widgets.removeAll()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GHarchiveToAzureBlobsETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
